{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Rashomon + 33/66/100% Features + SHAP & LIME\n", "\n", "**Plantilla reproducible** para estudiar el efecto Rashomon con tres presupuestos de atributos (33%, 66%, 100%),\n", "calculando el *\u03b5-Rashomon set* (WithinEpsilon), m\u00e9tricas de **volumen** (conteo), **complejidad**, y **estabilidad explicativa** con **SHAP** y **LIME**.\n", "\n", "**Qu\u00e9 hace:**\n", "1. Rankea atributos y define subconjuntos 33/66/100.\n", "2. Entrena m\u00faltiples modelos y selecciona los que est\u00e1n *within-\u03b5* del mejor (por AUC o p\u00e9rdida).\n", "3. Calcula m\u00e9tricas Rashomon (conteo, complejidad) y **estabilidad de explicaciones** (Kendall \u03c4, Jaccard@k, distancia entre vectores de importancias SHAP; varianza LIME).\n", "4. Grafica **Rashomon curves** (rendimiento vs complejidad) y **Stability vs Complejidad**.\n", "\n", "**Modelos incluidos (cuando las dependencias est\u00e1n disponibles):** QDA, XGB, GBM (sklearn), ERT, AdaBoost, MLP, Decision Tree, Logistic Regression, Random Forest, SVM, KNN, Bagging.\n", "\n", "> **Nota**: Este notebook intenta importar `shap` y `lime`. Si no los tienes, inst\u00e1lalos:\n", "```bash\n", "pip install shap lime\n", "```\n", "Adem\u00e1s, para XGBoost:\n", "```bash\n", "pip install xgboost\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ===============================\n", "# 0) Imports y utilidades\n", "# ===============================\n", "import numpy as np\n", "import pandas as pd\n", "from itertools import product\n", "from collections import defaultdict\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "from sklearn.datasets import make_classification\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n", "from sklearn.feature_selection import mutual_info_classif\n", "\n", "# Modelos cl\u00e1sicos\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import (\n", "    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier,\n", "    AdaBoostClassifier, BaggingClassifier\n", ")\n", "\n", "# XGBoost opcional\n", "try:\n", "    from xgboost import XGBClassifier\n", "    XGB_AVAILABLE = True\n", "except Exception:\n", "    XGB_AVAILABLE = False\n", "\n", "# SHAP & LIME opcionales\n", "try:\n", "    import shap\n", "    SHAP_AVAILABLE = True\n", "except Exception:\n", "    SHAP_AVAILABLE = False\n", "try:\n", "    from lime.lime_tabular import LimeTabularExplainer\n", "    LIME_AVAILABLE = True\n", "except Exception:\n", "    LIME_AVAILABLE = False\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "RANDOM_STATE = 42\n", "np.random.seed(RANDOM_STATE)\n", "\n", "print({'SHAP_AVAILABLE': SHAP_AVAILABLE, 'LIME_AVAILABLE': LIME_AVAILABLE, 'XGB_AVAILABLE': XGB_AVAILABLE})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Carga de datos\n", "Reemplaza la secci\u00f3n **B)** con tu dataset propio.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================\n", "# 1A) Dataset sint\u00e9tico de ejemplo (binario)\n", "# ============================================\n", "X, y = make_classification(\n", "    n_samples=4000, n_features=20, n_informative=8, n_redundant=4, n_repeated=0,\n", "    n_classes=2, weights=[0.55,0.45], class_sep=1.2, random_state=RANDOM_STATE\n", ")\n", "feature_names = [f'f{i}' for i in range(X.shape[1])]\n", "df = pd.DataFrame(X, columns=feature_names)\n", "df['target'] = y\n", "print(df.shape, df['target'].value_counts(normalize=True).round(3))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ==========================================================\n", "# 1B) (Opcional) Reemplaza por tu CSV\n", "# df = pd.read_csv('ruta/a/tu_dataset.csv')\n", "# y_col = 'target'  # Ajusta el nombre de la columna objetivo\n", "# feature_names = [c for c in df.columns if c != y_col]\n", "# X = df[feature_names].values\n", "# y = df[y_col].values\n", "# =========================================================="]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Train/Valid split y escalado opcional\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_valid, y_train, y_valid = train_test_split(\n", "    X, y, test_size=0.25, stratify=y, random_state=RANDOM_STATE\n", ")\n", "\n", "# Escalado para modelos sensibles a la escala\n", "scaler = StandardScaler()\n", "X_train_s = scaler.fit_transform(X_train)\n", "X_valid_s = scaler.transform(X_valid)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Ranking de atributos y subconjuntos 33/66/100\n", "Combinamos **mutual information** con importancias de un **\u00e1rbol extra-aleatorio** para robustez del ranking."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def feature_ranking(X_tr, y_tr, feature_names):\n", "    # Mutual Information (escala 0..1)\n", "    mi = mutual_info_classif(X_tr, y_tr, random_state=RANDOM_STATE)\n", "    # Importancia de ExtraTrees\n", "    et = ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE)\n", "    et.fit(X_tr, y_tr)\n", "    et_imp = et.feature_importances_\n", "    # Agregado\n", "    score = 0.5*mi + 0.5*et_imp\n", "    rk = pd.DataFrame({'feature': feature_names, 'score': score}).sort_values('score', ascending=False)\n", "    rk['Rank'] = np.arange(1, len(feature_names)+1)\n", "    return rk\n", "\n", "ranking_df = feature_ranking(X_train, y_train, feature_names)\n", "display(ranking_df.head(10))\n", "\n", "m = len(feature_names)\n", "k33 = max(1, int(np.ceil(0.33*m)))\n", "k66 = max(1, int(np.ceil(0.66*m)))\n", "S33 = ranking_df['feature'].iloc[:k33].tolist()\n", "S66 = ranking_df['feature'].iloc[:k66].tolist()\n", "S100 = ranking_df['feature'].tolist()\n", "S_map = {'33%': S33, '66%': S66, '100%': S100}\n", "print({'k33': k33, 'k66': k66, 'm': m})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Definici\u00f3n de modelos y grillas simples\n", "Cada modelo tiene una grilla corta de hiperpar\u00e1metros. El *Rashomon set* se construir\u00e1 con **reentrenos \u00d7 grilla**."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_models_dict():\n", "    models = {\n", "        'LogReg': (LogisticRegression(max_iter=500), {'C':[0.1,1,3], 'penalty':['l2'], 'solver':['lbfgs']}),\n", "        'QDA': (QDA(), {}),\n", "        'KNN': (KNeighborsClassifier(), {'n_neighbors':[3,5,11]}),\n", "        'SVM': (SVC(probability=True), {'C':[0.5,1,3], 'kernel':['rbf']}),\n", "        'DT': (DecisionTreeClassifier(random_state=RANDOM_STATE), {'max_depth':[2,3,4,5]}),\n", "        'RF': (RandomForestClassifier(random_state=RANDOM_STATE), {'n_estimators':[150], 'max_depth':[None,5,8]}),\n", "        'ERT': (ExtraTreesClassifier(random_state=RANDOM_STATE), {'n_estimators':[200], 'max_depth':[None,5,8]}),\n", "        'GBM': (GradientBoostingClassifier(random_state=RANDOM_STATE), {'n_estimators':[120], 'max_depth':[2,3]}),\n", "        'ADA': (AdaBoostClassifier(random_state=RANDOM_STATE), {'n_estimators':[150], 'learning_rate':[0.5,1.0]}),\n", "        'Bagging': (BaggingClassifier(random_state=RANDOM_STATE), {'n_estimators':[60]}),\n", "        'MLP': (MLPClassifier(max_iter=400, random_state=RANDOM_STATE), {'hidden_layer_sizes':[(50,), (50,50)], 'alpha':[0.0001, 0.001]}),\n", "    }\n", "    if XGB_AVAILABLE:\n", "        models['XGB'] = (XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE),\n", "                         {'n_estimators':[200], 'max_depth':[3,5], 'learning_rate':[0.1,0.2]})\n", "    return models\n", "\n", "models_dict = get_models_dict()\n", "list(models_dict.keys())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Bucle Rashomon: entrenamiento, selecci\u00f3n WithinEpsilon y complejidad\n", "Se define **m\u00e9trica objetivo** (por defecto AUC). Se calcula el mejor modelo por subconjunto y luego el **\u03b5\u2011level set**."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def model_complexity(model):\n", "    # Heur\u00edstica simple de complejidad por tipo\n", "    from sklearn.base import ClassifierMixin\n", "    label = model.__class__.__name__\n", "    try:\n", "        if label in ['DecisionTreeClassifier']:\n", "            return getattr(model, 'tree_', None).node_count if hasattr(model, 'tree_') else np.nan\n", "        if label in ['RandomForestClassifier','ExtraTreesClassifier','GradientBoostingClassifier','AdaBoostClassifier','BaggingClassifier']:\n", "            return len(getattr(model, 'estimators_', []))\n", "        if label in ['LogisticRegression']:\n", "            return np.count_nonzero(getattr(model, 'coef_', np.array([[]])))\n", "        if label in ['MLPClassifier']:\n", "            return sum(w.size for w in model.coefs_)\n", "        if label in ['SVC']:\n", "            return getattr(model, 'support_vectors_', np.array([])).shape[0]\n", "        if label in ['KNeighborsClassifier']:\n", "            return getattr(model, 'n_neighbors', np.nan)\n", "        if label in ['QuadraticDiscriminantAnalysis']:\n", "            return np.nan\n", "        if label in ['XGBClassifier']:\n", "            return getattr(model, 'n_estimators', np.nan)\n", "    except Exception:\n", "        return np.nan\n", "    return np.nan\n", "\n", "def fit_and_score(clf, Xtr, ytr, Xva, yva, use_scaled=False):\n", "    # algunos modelos usan X escalado\n", "    if use_scaled:\n", "        clf.fit(Xtr, ytr)\n", "        pred_proba = clf.predict_proba(Xva)[:,1] if hasattr(clf, 'predict_proba') else clf.decision_function(Xva)\n", "    else:\n", "        clf.fit(Xtr, ytr)\n", "        pred_proba = clf.predict_proba(Xva)[:,1] if hasattr(clf, 'predict_proba') else clf.decision_function(Xva)\n", "    auc = roc_auc_score(yva, pred_proba)\n", "    return auc\n", "\n", "def build_rashomon_set(S_name, features, epsilon=0.01, metric='AUC', seeds=[0,1,2]):\n", "    cols_idx = [feature_names.index(f) for f in features]\n", "    Xtr = X_train[:, cols_idx]\n", "    Xva = X_valid[:, cols_idx]\n", "    Xtr_s = scaler.fit_transform(Xtr)\n", "    Xva_s = scaler.transform(Xva)\n", "    records = []\n", "    best_auc = -np.inf\n", "\n", "    for mdl_name, (base, grid) in models_dict.items():\n", "        keys, values = list(grid.keys()), list(grid.values())\n", "        if len(keys)==0:\n", "            hp_list = [dict()]\n", "        else:\n", "            hp_list = [dict(zip(keys, v)) for v in product(*values)]\n", "        for hp in hp_list:\n", "            for seed in seeds:\n", "                # Clonar\n", "                import copy\n", "                clf = copy.deepcopy(base)\n", "                for k,v in hp.items():\n", "                    setattr(clf, k, v)\n", "                # Heur\u00edstica: usar escala para SVM/MLP/KNN/LogReg\n", "                use_scaled = mdl_name in ['SVM','MLP','KNN','LogReg']\n", "                auc = fit_and_score(clf, Xtr_s if use_scaled else Xtr, y_train, Xva_s if use_scaled else Xva, y_valid, use_scaled)\n", "                best_auc = max(best_auc, auc)\n", "                records.append({'Subset': S_name, 'Model': mdl_name, 'Params': hp, 'Seed': seed,\n", "                                'AUC': auc, 'Complexity': np.nan})\n", "                # Guardar complejidad luego de refit completo\n", "                try:\n", "                    clf.fit(Xtr_s if use_scaled else Xtr, y_train)\n", "                    comp = model_complexity(clf)\n", "                    records[-1]['Complexity'] = comp\n", "                except Exception:\n", "                    pass\n", "    res = pd.DataFrame(records)\n", "    res['BestAUC'] = best_auc\n", "    res['WithinEps'] = res['AUC'] >= (best_auc - epsilon)\n", "    return res\n", "\n", "epsilon = 0.01  # tolerancia en AUC absoluta\n", "results_all = []\n", "for key, feats in S_map.items():\n", "    res = build_rashomon_set(key, feats, epsilon=epsilon)\n", "    results_all.append(res)\n", "results_all = pd.concat(results_all, ignore_index=True)\n", "results_all.sort_values(['Subset','AUC'], ascending=[True,False]).head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Selecci\u00f3n del \u03b5\u2011Rashomon set y m\u00e9tricas de volumen + complejidad\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rashomon_sets = {s: results_all[(results_all['Subset']==s) & (results_all['WithinEps'])].copy() for s in S_map.keys()}\n", "summary = []\n", "for s, df_s in rashomon_sets.items():\n", "    cnt = len(df_s)\n", "    comp_mean = df_s['Complexity'].dropna().mean() if len(df_s['Complexity'].dropna())>0 else np.nan\n", "    comp_min = df_s['Complexity'].dropna().min() if len(df_s['Complexity'].dropna())>0 else np.nan\n", "    auc_best = df_s['AUC'].max() if len(df_s)>0 else np.nan\n", "    summary.append({'Subset': s, 'RashomonVolume(count)': cnt, 'ComplexityMean': comp_mean, 'ComplexityMin': comp_min, 'BestAUC_in_set': auc_best})\n", "summary_df = pd.DataFrame(summary).sort_values('Subset')\n", "summary_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) SHAP & LIME sobre el Rashomon set (estabilidad explicativa)\n", "Se calcula **SHAP global** (|SHAP| medio) por modelo y **LIME local** en un conjunto de instancias de validaci\u00f3n.\n", "\n", "> Si SHAP/LIME no est\u00e1n instalados, se omiten estas secciones.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_shap_global_vector(model, Xbg, Xeval, feature_list):\n", "    if not SHAP_AVAILABLE:\n", "        return None\n", "    try:\n", "        label = model.__class__.__name__\n", "        if label in ['RandomForestClassifier','ExtraTreesClassifier','GradientBoostingClassifier','DecisionTreeClassifier','AdaBoostClassifier','BaggingClassifier']:\n", "            explainer = shap.TreeExplainer(model)\n", "            sv = explainer.shap_values(Xeval)\n", "            if isinstance(sv, list):\n", "                sv = sv[1] if len(sv)>1 else sv[0]\n", "        elif label in ['LogisticRegression']:\n", "            explainer = shap.LinearExplainer(model, Xbg)\n", "            sv = explainer.shap_values(Xeval)\n", "        elif label in ['SVC','KNeighborsClassifier','QuadraticDiscriminantAnalysis','MLPClassifier'] or label.startswith('XGB'):\n", "            explainer = shap.KernelExplainer(model.predict_proba, Xbg)\n", "            sv = explainer.shap_values(Xeval, nsamples=200)\n", "            if isinstance(sv, list):\n", "                sv = sv[1] if len(sv)>1 else sv[0]\n", "        else:\n", "            return None\n", "        g = np.abs(sv).mean(axis=0)\n", "        return pd.Series(g, index=feature_list)\n", "    except Exception as e:\n", "        return None\n", "\n", "def kendall_tau_rank_sims(vectors_df):\n", "    # Kendall \u03c4 promedio entre rankings (global SHAP)\n", "    if vectors_df is None or len(vectors_df)==0:\n", "        return np.nan\n", "    from scipy.stats import kendalltau\n", "    cols = vectors_df.columns.tolist()\n", "    taus = []\n", "    for i in range(len(cols)):\n", "        for j in range(i+1, len(cols)):\n", "            r1 = vectors_df[cols[i]].rank(ascending=False)\n", "            r2 = vectors_df[cols[j]].rank(ascending=False)\n", "            tau, _ = kendalltau(r1, r2)\n", "            if not np.isnan(tau):\n", "                taus.append(tau)\n", "    return float(np.mean(taus)) if len(taus)>0 else np.nan\n", "\n", "def jaccard_topk(vectors_df, k=5):\n", "    if vectors_df is None or len(vectors_df)==0:\n", "        return np.nan\n", "    cols = vectors_df.columns.tolist()\n", "    from itertools import combinations\n", "    js = []\n", "    for a,b in combinations(cols, 2):\n", "        A = set(vectors_df[a].sort_values(ascending=False).index[:k])\n", "        B = set(vectors_df[b].sort_values(ascending=False).index[:k])\n", "        inter = len(A & B)\n", "        union = len(A | B)\n", "        js.append(inter/union if union>0 else np.nan)\n", "    return float(np.nanmean(js)) if len(js)>0 else np.nan\n", "\n", "def ed_l2_mean(vectors_df):\n", "    # Distancia L2 promedio entre vectores globales SHAP normalizados\n", "    if vectors_df is None or len(vectors_df)==0:\n", "        return np.nan\n", "    mats = vectors_df.to_numpy().T\n", "    mats = mats / (np.linalg.norm(mats, axis=1, keepdims=True)+1e-12)\n", "    ds = []\n", "    for i in range(mats.shape[0]):\n", "        for j in range(i+1, mats.shape[0]):\n", "            ds.append(np.linalg.norm(mats[i]-mats[j]))\n", "    return float(np.mean(ds)) if len(ds)>0 else np.nan\n", "\n", "def compute_explainability_stability_for_subset(S_name, features, max_models=8, k_top=5):\n", "    if not SHAP_AVAILABLE:\n", "        return {'Subset': S_name, 'KendallTau': np.nan, 'Jaccard@k': np.nan, 'ED_L2': np.nan}\n", "    cols_idx = [feature_names.index(f) for f in features]\n", "    Xtr = X_train[:, cols_idx]\n", "    Xva = X_valid[:, cols_idx]\n", "    # background para KernelExplainer\n", "    bg_rows = min(200, Xtr.shape[0])\n", "    Xbg = shap.sample(Xtr, bg_rows, random_state=RANDOM_STATE) if SHAP_AVAILABLE else Xtr[:bg_rows]\n", "    # tomar hasta max_models del rashomon set\n", "    df_set = rashomon_sets[S_name].copy()\n", "    if len(df_set)==0:\n", "        return {'Subset': S_name, 'KendallTau': np.nan, 'Jaccard@k': np.nan, 'ED_L2': np.nan}\n", "    df_set = df_set.sort_values('AUC', ascending=False).head(max_models)\n", "    vectors = {}\n", "    for idx, row in df_set.iterrows():\n", "        mdl = row['Model']\n", "        import copy\n", "        base, grid = models_dict[mdl]\n", "        clf = copy.deepcopy(base)\n", "        for k,v in row['Params'].items():\n", "            setattr(clf, k, v)\n", "        use_scaled = mdl in ['SVM','MLP','KNN','LogReg']\n", "        clf.fit((scaler.fit_transform(Xtr) if use_scaled else Xtr), y_train)\n", "        g = get_shap_global_vector(clf, Xbg, Xva, features)\n", "        if g is not None:\n", "            vectors[f\"{mdl}_{idx}\"] = g\n", "    if len(vectors)==0:\n", "        return {'Subset': S_name, 'KendallTau': np.nan, 'Jaccard@k': np.nan, 'ED_L2': np.nan}\n", "    V = pd.DataFrame(vectors)\n", "    return {\n", "        'Subset': S_name,\n", "        'KendallTau': kendall_tau_rank_sims(V),\n", "        'Jaccard@k': jaccard_topk(V, k=k_top),\n", "        'ED_L2': ed_l2_mean(V)\n", "    }\n", "\n", "stab_metrics = [compute_explainability_stability_for_subset(s, feats, max_models=8, k_top=5) for s,feats in S_map.items()]\n", "stab_df = pd.DataFrame(stab_metrics)\n", "stab_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) LIME (opcional) \u2014 varianza local\n", "Estimaci\u00f3n de **varianza LIME** para un pu\u00f1ado de instancias; \u00fatil como control de fragilidad local."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lime_local_variance(S_name, features, n_instances=20, repeats=5):\n", "    if not LIME_AVAILABLE:\n", "        return {'Subset': S_name, 'LIME_VarMean': np.nan}\n", "    cols_idx = [feature_names.index(f) for f in features]\n", "    Xtr = X_train[:, cols_idx]\n", "    Xva = X_valid[:, cols_idx]\n", "    df_set = rashomon_sets[S_name].sort_values('AUC', ascending=False)\n", "    if len(df_set)==0:\n", "        return {'Subset': S_name, 'LIME_VarMean': np.nan}\n", "    row = df_set.iloc[0]\n", "    mdl = row['Model']\n", "    base, grid = models_dict[mdl]\n", "    import copy\n", "    clf = copy.deepcopy(base)\n", "    for k,v in row['Params'].items():\n", "        setattr(clf, k, v)\n", "    use_scaled = mdl in ['SVM','MLP','KNN','LogReg']\n", "    Xtr_u = (scaler.fit_transform(Xtr) if use_scaled else Xtr)\n", "    Xva_u = (scaler.transform(Xva) if use_scaled else Xva)\n", "    clf.fit(Xtr_u, y_train)\n", "    expl = LimeTabularExplainer(Xtr_u, feature_names=features, class_names=['neg','pos'], discretize_continuous=True)\n", "    idxs = np.random.choice(np.arange(Xva_u.shape[0]), size=min(n_instances, Xva_u.shape[0]), replace=False)\n", "    vars_inst = []\n", "    for idx in idxs:\n", "        weights_mat = []\n", "        for r in range(repeats):\n", "            exp = expl.explain_instance(Xva_u[idx], clf.predict_proba, num_features=min(10, len(features)))\n", "            # Convertimos a vector ordenado por features\n", "            w = {k: v for k,v in exp.as_list()}\n", "            vec = np.array([w.get(f, 0.0) for f in features])\n", "            weights_mat.append(vec)\n", "        weights_mat = np.vstack(weights_mat)\n", "        vars_inst.append(weights_mat.var(axis=0).mean())\n", "    return {'Subset': S_name, 'LIME_VarMean': float(np.mean(vars_inst)) if len(vars_inst)>0 else np.nan}\n", "\n", "lime_df = pd.DataFrame([lime_local_variance(s, feats) for s,feats in S_map.items()])\n", "lime_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9) Gr\u00e1ficas (matplotlib)\n", "Reglas: **un gr\u00e1fico por figura** y sin estilos de color expl\u00edcitos."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 9.1 Rashomon curves: AUC vs Complejidad\n", "for s in S_map.keys():\n", "    df_s = rashomon_sets[s]\n", "    if len(df_s)==0:\n", "        continue\n", "    plt.figure(figsize=(6,4))\n", "    plt.scatter(df_s['Complexity'], df_s['AUC'])\n", "    plt.xlabel('Complejidad (heur\u00edstica)')\n", "    plt.ylabel('AUC (validaci\u00f3n)')\n", "    plt.title(f'Rashomon curve \u2014 Subconjunto {s}')\n", "    plt.show()\n", "\n", "# 9.2 Estabilidad vs Complejidad (ejemplo con KendallTau)\n", "if SHAP_AVAILABLE:\n", "    for s in S_map.keys():\n", "        df_s = rashomon_sets[s]\n", "        if len(df_s)==0:\n", "            continue\n", "        # aqu\u00ed graficamos AUC vs Complejidad; podr\u00edas colorear por rank stability si guardas por-modelo\n", "        plt.figure(figsize=(6,4))\n", "        plt.scatter(df_s['Complexity'], df_s['AUC'])\n", "        plt.xlabel('Complejidad (heur\u00edstica)')\n", "        plt.ylabel('AUC (validaci\u00f3n)')\n", "        plt.title(f'Estabilidad (proxy) vs Complejidad \u2014 {s}')\n", "        plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10) Exportar resultados\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["outdir = 'rashomon_outputs'\n", "os.makedirs(outdir, exist_ok=True)\n", "results_all.to_csv(os.path.join(outdir, 'models_grid_results.csv'), index=False)\n", "summary_df.to_csv(os.path.join(outdir, 'rashomon_summary.csv'), index=False)\n", "stab_df.to_csv(os.path.join(outdir, 'shap_stability.csv'), index=False)\n", "lime_df.to_csv(os.path.join(outdir, 'lime_variance.csv'), index=False)\n", "print('Archivos exportados en', outdir)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Notas finales\n", "- Ajusta `epsilon` para controlar el tama\u00f1o del \\(\u03b5\\)-Rashomon set.\n", "- Si cambias la m\u00e9trica objetivo (p. ej., `log_loss`), ajusta la selecci\u00f3n WithinEpsilon.\n", "- Puedes enriquecer la complejidad (conteo de nodos, profundidad exacta, #coeficientes no nulos) y a\u00f1adir **Rashomon ratio** si defines un denominador com\u00fan del espacio de hip\u00f3tesis.\n", "- Para datasets con categor\u00edas, agrega preprocesamiento (One-Hot/Ordinal) antes del ranking.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3"}}, "nbformat": 4, "nbformat_minor": 5}